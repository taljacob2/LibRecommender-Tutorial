{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/massquantity/LibRecommender/blob/master/examples/tutorial.ipynb)\n",
    "[![View in doc](https://img.shields.io/badge/document-tutorial-ffdfba)](https://librecommender.readthedocs.io/en/latest/tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "This tutorial will walk you through the comprehensive process of training a model in LibRecommender, i.e. **data processing -> feature engineering -> training -> evaluate -> save/load -> retrain**. We will use [Wide & Deep](https://arxiv.org/pdf/1606.07792.pdf) as the example algorithm. \n",
    "\n",
    "First make sure the latest LibRecommender has been installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U LibRecommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For how to deploy a trained model in LibRecommender, see [Serving Guide](https://librecommender.readthedocs.io/en/latest/serving_guide/python.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**NOTE**: If you encounter errors like `Variables already exist, disallowed...`, just call `tf.compat.v1.reset_default_graph()` first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this tutorial we willl use the [MovieLens 1M](https://grouplens.org/datasets/movielens/1m/) dataset. The following code will load the data into `pandas.DataFrame` format. If the data does not exist locally, it will be downloaded at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_ml_1m():\n",
    "    # download and extract zip file\n",
    "    tf.keras.utils.get_file(\n",
    "        \"ml-1m.zip\", \n",
    "        \"http://files.grouplens.org/datasets/movielens/ml-1m.zip\", \n",
    "        cache_dir=\".\", \n",
    "        cache_subdir=\".\", \n",
    "        extract=True,\n",
    "    )\n",
    "    # read and merge data into same table\n",
    "    cur_path = Path(\".\").absolute()\n",
    "    ratings = pd.read_csv(\n",
    "        cur_path / \"ml-1m\" / \"ratings.dat\", \n",
    "        sep=\"::\", \n",
    "        usecols=[0, 1, 2, 3], \n",
    "        names=[\"user\", \"item\", \"rating\", \"time\"],\n",
    "    )\n",
    "    users = pd.read_csv(\n",
    "        cur_path / \"ml-1m\" / \"users.dat\", \n",
    "        sep=\"::\",\n",
    "        usecols=[0, 1, 2, 3], \n",
    "        names=[\"user\", \"sex\", \"age\", \"occupation\"],\n",
    "    )\n",
    "    items = pd.read_csv(\n",
    "        cur_path / \"ml-1m\" / \"movies.dat\", \n",
    "        sep=\"::\",\n",
    "        usecols=[0, 2], \n",
    "        names=[\"item\", \"genre\"],\n",
    "        encoding=\"iso-8859-1\",\n",
    "    )\n",
    "    items[[\"genre1\", \"genre2\", \"genre3\"]] = (\n",
    "        items[\"genre\"].str.split(r\"|\", expand=True).fillna(\"missing\").iloc[:, :3]\n",
    "    )\n",
    "    items.drop(\"genre\", axis=1, inplace=True)\n",
    "    data = ratings.merge(users, on=\"user\").merge(items, on=\"item\")\n",
    "    data.rename(columns={\"rating\": \"label\"}, inplace=True)\n",
    "    # random shuffle data\n",
    "    data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (1000209, 10)\n"
     ]
    }
   ],
   "source": [
    "data = load_ml_1m()\n",
    "print(\"data shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>label</th>\n",
       "      <th>time</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>genre1</th>\n",
       "      <th>genre2</th>\n",
       "      <th>genre3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>897381</th>\n",
       "      <td>4127</td>\n",
       "      <td>3699</td>\n",
       "      <td>3</td>\n",
       "      <td>965356931</td>\n",
       "      <td>M</td>\n",
       "      <td>50</td>\n",
       "      <td>17</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615720</th>\n",
       "      <td>3119</td>\n",
       "      <td>1240</td>\n",
       "      <td>4</td>\n",
       "      <td>969393440</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>Action</td>\n",
       "      <td>Sci-Fi</td>\n",
       "      <td>Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28966</th>\n",
       "      <td>2966</td>\n",
       "      <td>2581</td>\n",
       "      <td>4</td>\n",
       "      <td>971137483</td>\n",
       "      <td>F</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>Romance</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691341</th>\n",
       "      <td>1938</td>\n",
       "      <td>2105</td>\n",
       "      <td>4</td>\n",
       "      <td>974695701</td>\n",
       "      <td>F</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>Action</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685815</th>\n",
       "      <td>1522</td>\n",
       "      <td>2968</td>\n",
       "      <td>4</td>\n",
       "      <td>974745539</td>\n",
       "      <td>M</td>\n",
       "      <td>35</td>\n",
       "      <td>20</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>Fantasy</td>\n",
       "      <td>Sci-Fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925314</th>\n",
       "      <td>2816</td>\n",
       "      <td>480</td>\n",
       "      <td>4</td>\n",
       "      <td>972682268</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>Action</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>Sci-Fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690597</th>\n",
       "      <td>5780</td>\n",
       "      <td>788</td>\n",
       "      <td>4</td>\n",
       "      <td>958154511</td>\n",
       "      <td>M</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>Fantasy</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383093</th>\n",
       "      <td>1110</td>\n",
       "      <td>593</td>\n",
       "      <td>4</td>\n",
       "      <td>974919464</td>\n",
       "      <td>F</td>\n",
       "      <td>56</td>\n",
       "      <td>6</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686437</th>\n",
       "      <td>4829</td>\n",
       "      <td>1653</td>\n",
       "      <td>5</td>\n",
       "      <td>1007833084</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Sci-Fi</td>\n",
       "      <td>Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246925</th>\n",
       "      <td>4995</td>\n",
       "      <td>2985</td>\n",
       "      <td>3</td>\n",
       "      <td>962596185</td>\n",
       "      <td>M</td>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>Action</td>\n",
       "      <td>Crime</td>\n",
       "      <td>Sci-Fi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user  item  label        time sex  age  occupation     genre1  \\\n",
       "897381  4127  3699      3   965356931   M   50          17  Adventure   \n",
       "615720  3119  1240      4   969393440   M   45           1     Action   \n",
       "28966   2966  2581      4   971137483   F   25           4     Comedy   \n",
       "691341  1938  2105      4   974695701   F   25           2     Action   \n",
       "685815  1522  2968      4   974745539   M   35          20  Adventure   \n",
       "925314  2816   480      4   972682268   M   25           7     Action   \n",
       "690597  5780   788      4   958154511   M   18          17     Comedy   \n",
       "383093  1110   593      4   974919464   F   56           6      Drama   \n",
       "686437  4829  1653      5  1007833084   M   25           0      Drama   \n",
       "246925  4995  2985      3   962596185   M   50          20     Action   \n",
       "\n",
       "           genre2    genre3  \n",
       "897381      Drama   Romance  \n",
       "615720     Sci-Fi  Thriller  \n",
       "28966     Romance   missing  \n",
       "691341  Adventure   Fantasy  \n",
       "685815    Fantasy    Sci-Fi  \n",
       "925314  Adventure    Sci-Fi  \n",
       "690597    Fantasy   Romance  \n",
       "383093   Thriller   missing  \n",
       "686437     Sci-Fi  Thriller  \n",
       "246925      Crime    Sci-Fi  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[random.choices(range(len(data)), k=10)]  # randomly select 10 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we have about 1 million data. In order to perform evaluation after training, we need to split the data into train, eval and test data first. In this tutorial we will simply use `random_split`. For other ways of splitting data, see [Data Processing](https://librecommender.readthedocs.io/en/latest/user_guide/data_processing.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "**For now, We will only use first half data for training. Later we will use the rest data to retrain the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Process Data & Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from libreco.data import random_split\n",
    "\n",
    "# split data into three folds for training, evaluating and testing\n",
    "first_half_data = data[: (len(data) // 2)]\n",
    "train_data, eval_data, test_data = random_split(first_half_data, multi_ratios=[0.8, 0.1, 0.1], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The data contains some categorical features such as \"sex\" and \"genre\", as well as a numerical feature \"age\". In LibRecommender we use `sparse_col` to represent categorical features and `dense_col` to represent numerical features. So one should specify the column information and then use `DatasetFeat.build_*` functions to process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first half data shape: (500104, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"first half data shape:\", first_half_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from libreco.data import DatasetFeat\n",
    "\n",
    "sparse_col = [\"sex\", \"occupation\", \"genre1\", \"genre2\", \"genre3\"]\n",
    "dense_col = [\"age\"]\n",
    "user_col = [\"sex\", \"age\", \"occupation\"]\n",
    "item_col = [\"genre1\", \"genre2\", \"genre3\"]\n",
    "\n",
    "train_data, data_info = DatasetFeat.build_trainset(train_data, user_col, item_col, sparse_col, dense_col)\n",
    "eval_data = DatasetFeat.build_evalset(eval_data)\n",
    "test_data = DatasetFeat.build_testset(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\"user_col\" means features belong to user, and \"item_col\" means features belong to item. Note that the column numbers should match, i.e. `len(sparse_col) + len(dense_col) == len(user_col) + len(item_col)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_users: 6040, n_items: 3576, data density: 1.8523 %\n"
     ]
    }
   ],
   "source": [
    "print(data_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now with all the data and features prepared, we can start training the model! \n",
    "\n",
    "Since as its name suggests, the `Wide & Deep` algorithm has wide and deep parts, and they use different optimizers. So we should specify the learning rate separately by using a dict: `{\"wide\": 0.01, \"deep\": 3e-4}`. For other model hyperparameters, see API reference of [WideDeep](https://librecommender.readthedocs.io/en/latest/api/algorithms/wide_deep.html).\n",
    "\n",
    "In this example we treat all the samples in data as positive samples, and perform negative sampling. This is called \"implicit data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from libreco.algorithms import WideDeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start time: \u001B[35m2023-04-06 15:12:45\u001B[0m\n",
      "WARNING:tensorflow:From /home/massquantity/miniconda3/lib/python3.8/site-packages/keras/layers/normalization/batch_normalization.py:562: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 15:12:45.758683: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-06 15:12:45,844 - WARNING - From /home/massquantity/miniconda3/lib/python3.8/site-packages/keras/layers/normalization/batch_normalization.py:562: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total params: \u001B[33m192,413\u001B[0m | embedding params: \u001B[33m165,109\u001B[0m | network params: \u001B[33m27,304\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 15:12:46.174116: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "train: 100%|██████████████████████████████████████████████████████| 391/391 [00:02<00:00, 134.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 elapsed: 2.905s\n",
      "\t \u001B[32mtrain_loss: 0.959\u001B[0m\n",
      "random neg item sampling elapsed: 0.024s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|███████████████████████████████████████████████| 13/13 [00:00<00:00, 143.54it/s]\n",
      "eval_listwise: 100%|████████████████████████████████████████████| 2797/2797 [00:09<00:00, 287.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval log_loss: 0.5823\n",
      "\t eval roc_auc: 0.8032\n",
      "\t eval precision@10: 0.0236\n",
      "\t eval recall@10: 0.0339\n",
      "\t eval ndcg@10: 0.1001\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████████████████████████████████████████████████| 391/391 [00:02<00:00, 156.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 elapsed: 2.508s\n",
      "\t \u001B[32mtrain_loss: 0.499\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|███████████████████████████████████████████████| 13/13 [00:00<00:00, 235.78it/s]\n",
      "eval_listwise: 100%|████████████████████████████████████████████| 2797/2797 [00:10<00:00, 256.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval log_loss: 0.4769\n",
      "\t eval roc_auc: 0.8488\n",
      "\t eval precision@10: 0.0332\n",
      "\t eval recall@10: 0.0523\n",
      "\t eval ndcg@10: 0.1376\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "model = WideDeep(\n",
    "    task=\"ranking\",\n",
    "    data_info=data_info,\n",
    "    embed_size=16,\n",
    "    n_epochs=2,\n",
    "    loss_type=\"cross_entropy\",\n",
    "    lr={\"wide\": 0.05, \"deep\": 7e-4},\n",
    "    batch_size=2048,\n",
    "    use_bn=True,\n",
    "    hidden_units=(128, 64, 32),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_data,\n",
    "    neg_sampling=True,  # perform negative sampling on training and eval data\n",
    "    verbose=2,\n",
    "    shuffle=True,\n",
    "    eval_data=eval_data,\n",
    "    metrics=[\"loss\", \"roc_auc\", \"precision\", \"recall\", \"ndcg\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We've trained the model for 2 epochs and evaluated the performance on the eval data during training. Next we can evaluate on the *independent* test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random neg item sampling elapsed: 0.025s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|███████████████████████████████████████████████| 13/13 [00:00<00:00, 219.84it/s]\n",
      "eval_listwise: 100%|████████████████████████████████████████████| 2834/2834 [00:10<00:00, 278.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.4782908669403157,\n",
       " 'roc_auc': 0.8483713737644527,\n",
       " 'precision': 0.031268748897123694,\n",
       " 'recall': 0.04829594849021039,\n",
       " 'ndcg': 0.12866793895121623}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from libreco.evaluation import evaluate\n",
    "\n",
    "evaluate(\n",
    "    model=model, \n",
    "    data=test_data, \n",
    "    neg_sampling=True,  # perform negative sampling on test data\n",
    "    metrics=[\"loss\", \"roc_auc\", \"precision\", \"recall\", \"ndcg\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Make Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The recommend part is pretty straightforward. You can make recommendation for one user or a batch of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([ 364, 3751, 2858])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.recommend_user(user=1, n_rec=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([ 364, 3751, 2858]),\n",
       " 2: array([1617,  608,  912]),\n",
       " 3: array([ 589, 2571, 1200])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.recommend_user(user=[1, 2, 3], n_rec=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can also make recommdation based on specific user features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([2716,  589, 2571])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.recommend_user(user=1, n_rec=3, user_feats={\"sex\": \"M\", \"age\": 33})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([2858, 1210, 1580])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.recommend_user(user=1, n_rec=3, user_feats={\"occupation\": 17})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Save, Load and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "When saving the model, we should also save the `data_info` for feature information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_info.save(\"model_path\", model_name=\"wide_deep\")\n",
    "model.save(\"model_path\", model_name=\"wide_deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then we can load the model and make recommendation again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()  # need to reset graph in TensorFlow1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total params: \u001B[33m192,413\u001B[0m | embedding params: \u001B[33m165,109\u001B[0m | network params: \u001B[33m27,304\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: array([ 364, 3751, 2858])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from libreco.data import DataInfo\n",
    "\n",
    "loaded_data_info = DataInfo.load(\"model_path\", model_name=\"wide_deep\")\n",
    "loaded_model = WideDeep.load(\"model_path\", model_name=\"wide_deep\", data_info=loaded_data_info)\n",
    "loaded_model.recommend_user(user=1, n_rec=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Retrain the Model with New Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Remember that we split the original `MovieLens 1M` data into two parts in the first place? We will treat the **second half** of the data as our new data and retrain the saved model with it. In real-world recommender systems, data may be generated every day, so it is inefficient to train the model from scratch every time we get some new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "second_half_data = data[(len(data) // 2) :]\n",
    "train_data, eval_data = random_split(second_half_data, multi_ratios=[0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second half data shape: (500105, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"second half data shape:\", second_half_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The data processing is similar, except that we should use `merge_trainset()` and `merge_evalset()` in DatasetFeat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The purpose of these functions is combining information from old data with that from new data, especially for the possible new users/items from new data. For more details, see [Model Retrain](https://librecommender.readthedocs.io/en/latest/user_guide/model_retrain.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pass `loaded_data_info` and get `new_data_info`\n",
    "train_data, new_data_info = DatasetFeat.merge_trainset(train_data, loaded_data_info, merge_behavior=True)\n",
    "eval_data = DatasetFeat.merge_evalset(eval_data, new_data_info)  # use new_data_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then we construct a new model, and call `rebuild_model` method to assign the old variables into the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()  # need to reset graph in TensorFlow1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total params: \u001B[33m194,164\u001B[0m | embedding params: \u001B[33m166,860\u001B[0m | network params: \u001B[33m27,304\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "new_model = WideDeep(\n",
    "    task=\"ranking\",\n",
    "    data_info=new_data_info,  # pass new_data_info\n",
    "    embed_size=16,\n",
    "    n_epochs=2,\n",
    "    loss_type=\"cross_entropy\",\n",
    "    lr={\"wide\": 0.01, \"deep\": 1e-4},\n",
    "    batch_size=2048,\n",
    "    use_bn=True,\n",
    "    hidden_units=(128, 64, 32),\n",
    ")\n",
    "\n",
    "new_model.rebuild_model(path=\"model_path\", model_name=\"wide_deep\", full_assign=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, the training and recommendation parts are the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start time: \u001B[35m2023-04-06 15:18:29\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████████████████████████████████████████████████| 391/391 [00:02<00:00, 136.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 elapsed: 2.867s\n",
      "\t \u001B[32mtrain_loss: 0.4867\u001B[0m\n",
      "random neg item sampling elapsed: 0.058s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|███████████████████████████████████████████████| 25/25 [00:00<00:00, 175.29it/s]\n",
      "eval_listwise: 100%|████████████████████████████████████████████| 2981/2981 [00:11<00:00, 262.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval log_loss: 0.4482\n",
      "\t eval roc_auc: 0.8708\n",
      "\t eval precision@10: 0.0985\n",
      "\t eval recall@10: 0.0710\n",
      "\t eval ndcg@10: 0.3062\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████████████████████████████████████████████████| 391/391 [00:02<00:00, 141.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 elapsed: 2.770s\n",
      "\t \u001B[32mtrain_loss: 0.472\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|███████████████████████████████████████████████| 25/25 [00:00<00:00, 214.44it/s]\n",
      "eval_listwise: 100%|████████████████████████████████████████████| 2981/2981 [00:10<00:00, 275.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval log_loss: 0.4416\n",
      "\t eval roc_auc: 0.8741\n",
      "\t eval precision@10: 0.1031\n",
      "\t eval recall@10: 0.0738\n",
      "\t eval ndcg@10: 0.3168\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "new_model.fit(\n",
    "    train_data, \n",
    "    neg_sampling=True,\n",
    "    verbose=2, \n",
    "    shuffle=True, \n",
    "    eval_data=eval_data,\n",
    "    metrics=[\"loss\", \"roc_auc\", \"precision\", \"recall\", \"ndcg\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([ 364, 2858, 1210])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.recommend_user(user=1, n_rec=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([ 364, 2858, 1210]),\n",
       " 2: array([ 608, 1617, 1233]),\n",
       " 3: array([ 589, 2571, 1387])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.recommend_user(user=[1, 2, 3], n_rec=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**This completes our tutorial!**\n",
    "\n",
    "+ For more examples, see the [examples](https://github.com/massquantity/LibRecommender/tree/master/examples) folder on GitHub. \n",
    "\n",
    "+ For more usages, please head to [User Guide](https://librecommender.readthedocs.io/en/latest/user_guide/index.html).\n",
    "\n",
    "+ For serving a trained model, please head to [Python Serving Guide](https://librecommender.readthedocs.io/en/latest/serving_guide/python.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}